{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q Bio pandas tqdm torch\n",
        "# ! pip install torch \n",
        "# --index-url https://download.pytorch.org/whl/gpu\n",
        "!hostname\n",
        "!nvidia-smi\n",
        "# !cat /usr/local/cuda/version.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0SpDuJl4dLT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from Bio import SeqIO\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import io \n",
        "import base64\n",
        "import torch\n",
        "\n",
        "SCRATCH_DIR = '/gpfs/scratch/petera17'\n",
        "DATASETS_DIR = f'{SCRATCH_DIR}/scratch/datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Ref and Alt Sequences to ClinVar Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Clinvar\n",
        "clinvar_df = pd.read_csv(\"/gpfs/scratch/petera17/scratch/datasets/ClinVar_benchmark_all_PB.csv\")\n",
        "\n",
        "# Read Fasta\n",
        "fasta_file = f\"{DATASETS_DIR}/genome.fa\"\n",
        "parsed_fasta_iter = SeqIO.parse(fasta_file, \"fasta\")\n",
        "record_dict = {}\n",
        "try:\n",
        "    while(True):\n",
        "        seq_req = next(parsed_fasta_iter)\n",
        "        record_dict[seq_req.id] = seq_req\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "seq_dict = {}\n",
        "for i in list(range(1,23)) + ['Y','X']:\n",
        "  seq_dict[str(i)] = record_dict[f'chr{i}']\n",
        "\n",
        "# Append Fasta seq to clinvar\n",
        "window = 4000\n",
        "clinvar_df[\"SEQ\"] = clinvar_df.apply(\n",
        "    lambda x: \"\".join(\n",
        "        seq_dict[x['CHROM']].seq[int(x['POS'])-window:int(x['POS'])+window])\n",
        "    , axis=1)\n",
        "\n",
        "#Save\n",
        "# clinvar_df.to_csv(f'{DATASETS_DIR}/coding_with_seq_df.csv', index=False)\n",
        "\n",
        "def replace_3999th_char(seq, alt):\n",
        "    if pd.isna(seq) or pd.isna(alt) or len(seq) < 3999:\n",
        "        return seq  # return unchanged if too short or missing\n",
        "    return seq[:3999] + alt + seq[4000:]\n",
        "\n",
        "# Apply the function row-wise\n",
        "clinvar_df['ALT_SEQ'] = clinvar_df.apply(lambda row: replace_3999th_char(row['SEQ'], row['ALT']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clinvar_df = pd.read_csv(f'{DATASETS_DIR}/clinvar_w_seq_202504.csv')\n",
        "clinvar_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarize and Sample from Variant Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "amino_acids = {'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I',\n",
        "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V'}\n",
        "\n",
        "coding_df = clinvar_df[clinvar_df[\"ClinVarName_coding_sequence\"] == 1]\n",
        "\n",
        "non_coding_df = clinvar_df[clinvar_df[\"ClinVarName_coding_sequence\"] == 0]\n",
        "\n",
        "def classify_variant(row):\n",
        "    if row['ClinVarName_AAREF'] != row['ClinVarName_AAALT'] and row['ClinVarName_AAALT'] in amino_acids:\n",
        "        return 'missense'\n",
        "    if row['ClinVarName_AAALT'] == '*':\n",
        "        return 'stop_gain'\n",
        "    if row['ClinVarName_AAALT'] == \"=\":\n",
        "        return 'synonymous'\n",
        "    return 'other'\n",
        "\n",
        "coding_df['variant_category'] = coding_df.apply(classify_variant, axis=1)\n",
        "non_coding_df['variant_category'] = \"non_coding\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Binary flags to check in non_coding_df\n",
        "non_coding_flags = [\n",
        "    'five_prime_UTR',\n",
        "    'three_prime_UTR',\n",
        "    'mRNA_intron',\n",
        "    'mRNA_splice',\n",
        "    'snRNA',\n",
        "    'snRNA_exon',\n",
        "    'snoRNA',\n",
        "    'snoRNA_exon',\n",
        "]\n",
        "\n",
        "# Star levels (exact match)\n",
        "star_levels = [0, 1, 2, 3, 4]\n",
        "columns = ['Total', 'NaN stars'] + [f'{s} stars' for s in star_levels]\n",
        "\n",
        "# Helper function\n",
        "def get_benign_pathogenic_string(df):\n",
        "    benign = (df['INFO'] == 0).sum()\n",
        "    pathogenic = (df['INFO'] == 1).sum()\n",
        "    return f\"{pathogenic} / {benign}\"\n",
        "\n",
        "# Build result\n",
        "result_nc = {}\n",
        "\n",
        "for flag in non_coding_flags:\n",
        "    row = {}\n",
        "\n",
        "    flagged_df = non_coding_df[non_coding_df[flag] == 1]\n",
        "\n",
        "    # Total\n",
        "    row['Total'] = get_benign_pathogenic_string(flagged_df)\n",
        "\n",
        "    # NaN stars\n",
        "    nan_df = flagged_df[flagged_df['ClinVar_gold_stars'].isna()]\n",
        "    row['NaN stars'] = get_benign_pathogenic_string(nan_df)\n",
        "\n",
        "    # Exact star levels\n",
        "    for s in star_levels:\n",
        "        star_df = flagged_df[flagged_df['ClinVar_gold_stars'] == s]\n",
        "        row[f'{s} stars'] = get_benign_pathogenic_string(star_df)\n",
        "\n",
        "    result_nc[flag] = row\n",
        "\n",
        "# Create DataFrame\n",
        "summary_nc_df = pd.DataFrame.from_dict(result_nc, orient='index', columns=columns)\n",
        "summary_nc_df.index.name = 'non_coding_category'\n",
        "\n",
        "# Display\n",
        "print(summary_nc_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare gold star thresholds\n",
        "thresholds = [1, 2, 3, 4]\n",
        "combined_counts = {}\n",
        "\n",
        "# Helper function to get benign/pathogenic counts\n",
        "def count_benign_pathogenic(df):\n",
        "    counts = {}\n",
        "    for category in df['variant_category'].unique():\n",
        "        sub_df = df[df['variant_category'] == category]\n",
        "        benign = (sub_df['INFO'] == 0).sum()\n",
        "        pathogenic = (sub_df['INFO'] == 1).sum()\n",
        "        counts[category] = f\"{benign} / {pathogenic}\"\n",
        "    return pd.Series(counts)\n",
        "\n",
        "# Total\n",
        "combined_counts['Total'] = count_benign_pathogenic(coding_df)\n",
        "\n",
        "# NaN stars\n",
        "nan_df = coding_df[coding_df['ClinVar_gold_stars'].isna()]\n",
        "combined_counts['NaN stars'] = count_benign_pathogenic(nan_df)\n",
        "\n",
        "# Thresholds\n",
        "for t in thresholds:\n",
        "    filtered = coding_df[coding_df['ClinVar_gold_stars'] >= t]\n",
        "    combined_counts[f'â‰¥{t} stars'] = count_benign_pathogenic(filtered)\n",
        "\n",
        "# Combine into DataFrame\n",
        "summary_df = pd.DataFrame(combined_counts).fillna(\"0 / 0\")\n",
        "\n",
        "# Display\n",
        "print(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "sampled_dfs = {}\n",
        "\n",
        "# Helper: get 100 benign / 100 pathogenic samples per variant_category\n",
        "def get_balanced_samples(df):\n",
        "    samples = []\n",
        "    for category, group in df.groupby('variant_category'):\n",
        "        benign = group[group['INFO'] == 0]\n",
        "        pathogenic = group[group['INFO'] == 1]\n",
        "        \n",
        "        if len(benign) >= 100: \n",
        "            benign_sample = benign.sample(n=100, random_state=42)\n",
        "            samples.append(benign_sample)\n",
        "        else:\n",
        "            samples.append(benign)\n",
        "\n",
        "        if len(pathogenic) >= 100:\n",
        "            pathogenic_sample = pathogenic.sample(n=100, random_state=42)\n",
        "            samples.append(pathogenic_sample)\n",
        "        else:\n",
        "            samples.append(pathogenic)\n",
        "\n",
        "    if samples:\n",
        "        result = pd.concat(samples).reset_index(drop=True)\n",
        "        return result\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Handle exact gold star levels\n",
        "# for star in gold_star_levels:\n",
        "two_star_coding_df = coding_df[coding_df[\"ClinVar_gold_stars\"] >= 2]\n",
        "sampled_coding_df = get_balanced_samples(coding_df)\n",
        "\n",
        "# Optional: inspect sample size breakdown\n",
        "print(sampled_coding_df.groupby(['variant_category', 'INFO']).size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "non_coding_flags = [\n",
        "    'five_prime_UTR',\n",
        "    'three_prime_UTR',\n",
        "    'mRNA_intron',\n",
        "    'mRNA_splice',\n",
        "    'snRNA',\n",
        "    'snRNA_exon',\n",
        "    'snoRNA',\n",
        "    'snoRNA_exon',\n",
        "]\n",
        "\n",
        "sampled_dfs = {}\n",
        "\n",
        "# Helper to sample 100 benign + 100 pathogenic per flag\n",
        "def get_balanced_samples(df, flag):\n",
        "    flagged = df[df[flag] == 1]\n",
        "    benign = flagged[flagged['INFO'] == 0]\n",
        "    pathogenic = flagged[flagged['INFO'] == 1]\n",
        "    \n",
        "    samples = []\n",
        "\n",
        "    if len(benign) > 100:\n",
        "        samples.append(benign.sample(n=100, random_state=42))\n",
        "    else:\n",
        "        samples.append(benign)\n",
        "\n",
        "    if len(pathogenic) > 100:\n",
        "        samples.append(pathogenic.sample(n=100, random_state=42))\n",
        "    else:\n",
        "        samples.append(pathogenic)\n",
        "\n",
        "    if samples:\n",
        "        combined = pd.concat(samples)\n",
        "        combined['non_coding_category'] = flag\n",
        "        return combined\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Sample for each non-coding flag\n",
        "for flag in non_coding_flags:\n",
        "    sampled = get_balanced_samples(non_coding_df, flag)\n",
        "    if not sampled.empty:\n",
        "        sampled_dfs[flag] = sampled\n",
        "\n",
        "# Combine all into a single DataFrame\n",
        "non_coding_samples = pd.concat(sampled_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Optional: Inspect result\n",
        "print(non_coding_samples['non_coding_category'].value_counts())\n",
        "print(non_coding_samples.groupby(['non_coding_category', 'INFO']).size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_coding_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine coding and non-coding sampled datasets\n",
        "final_combined_samples = pd.concat(\n",
        "    [sampled_coding_df, non_coding_samples],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "final_combined_samples.to_csv(f\"{DATASETS_DIR}/clinvar_w_seq_202504_sampled.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_combined_samples = pd.read_csv(f\"{DATASETS_DIR}/clinvar_w_seq_202504_sampled.csv\")\n",
        "final_combined_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Score Sequences EVO2 40B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Vortex tokenizer just does this:\n",
        "# https://github.com/Zymrael/vortex/blob/f243e8ec5da8374be082a77056c0a447e7fa9231/vortex/model/tokenizer.py#L161C1-L161C9\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def get_logits(sequence: str) -> torch.Tensor:\n",
        "    r = requests.post(\n",
        "        url=\"http://10.189.26.12:30100/biology/arc/evo2/forward\",\n",
        "        json={\n",
        "            \"sequence\": sequence,\n",
        "            \"output_layers\":[\"unembed\"]\n",
        "        },\n",
        "    )\n",
        "    response = json.loads(r.content.decode('utf-8'))\n",
        "    if response.get(\"error\", None):\n",
        "        print(response[\"error\"])\n",
        "    data_string = response['data']\n",
        "\n",
        "    decoded_data = base64.b64decode(data_string)\n",
        "    buffer = io.BytesIO(decoded_data)\n",
        "    npz_data = np.load(buffer)\n",
        "    logits = np.array(npz_data['unembed.output'])\n",
        "    return torch.tensor(logits)\n",
        "\n",
        "def tokenize(seq) -> List:\n",
        "    tokens = list(np.fromstring(seq, dtype=np.uint8))\n",
        "    return tokens\n",
        "\n",
        "def score_alt_nucleutide(sequences: List[str], reduce_method: str = 'mean', device='cpu'):\n",
        "\n",
        "  # Tokenize\n",
        "  input_ids = torch.tensor([tokenize(sequence) for sequence in sequences], dtype=torch.int64, device=device)\n",
        "\n",
        "  # Get Logits\n",
        "  with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "      logits_list = list(executor.map(get_logits, sequences))\n",
        "      logits = torch.cat(logits_list, dim=0)  # n_batch_size x n_seq_length x 512\n",
        "  \n",
        "  # Apply Softmax\n",
        "  softmax_logprobs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "  # Get likelihoods for the actual input sequences\n",
        "  sequence_logprobs = torch.gather(\n",
        "      softmax_logprobs,       # Gather likelihoods...\n",
        "      2,                      # along the vocab dimension...\n",
        "      input_ids.unsqueeze(-1)\n",
        "  ).squeeze(-1)\n",
        "\n",
        "#   return sequence_logprobs\n",
        "  \n",
        "  if reduce_method == 'sum': # PLL\n",
        "     reduce_func = np.sum\n",
        "  elif reduce_method == 'mean': # mean PLL\n",
        "     reduce_func = np.mean\n",
        "  else:\n",
        "     raise ValueError(f'Invalid reduce_method {reduce_method}')\n",
        "\n",
        "  return [\n",
        "     reduce_func(sequence_logprobs[idx].cpu().numpy())\n",
        "     for idx in range(len(sequences))\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clinvar_filtered_df = clinvar_df[clinvar_df['ALT'].str.contains(r'[ACTG]', regex=True)]\n",
        "# clinvar_filtered_df['ALT'].unique()\n",
        "# coding_1000_df = clinvar_filtered_df.sample(1000)\n",
        "# final_combined_samples = pd.read_csv(f'{DATASETS_DIR}/clinvar_w_seq_202504_sampled_scored.csv')\n",
        "final_combined_samples.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Assuming coding_df is your DataFrame and score_alt_nucleutide is defined\n",
        "final_combined_samples['SEQ_SCORE'] = 0.0\n",
        "final_combined_samples['ALT_SEQ_SCORE'] = 0.0\n",
        "\n",
        "batch_size = 100\n",
        "start_index = 0\n",
        "for i in tqdm(range(start_index, len(final_combined_samples), batch_size)):\n",
        "  rows = final_combined_samples.iloc[i:i+batch_size]\n",
        "  final_combined_samples.loc[rows.index, 'SEQ_SCORE'] = score_alt_nucleutide(rows['SEQ'].values)\n",
        "  final_combined_samples.loc[rows.index, 'ALT_SEQ_SCORE'] = score_alt_nucleutide(rows['ALT_SEQ'].values)\n",
        "  final_combined_samples.to_csv(f'{DATASETS_DIR}/clinvar_w_seq_202504_sampled_scored.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(final_combined_samples[\"SEQ_SCORE\"] != 0).sum()\n",
        "# coding_with_seq_df[\"ID\"].iloc[0:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROCAUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "final_combined_samples['PLL_SCORE'] = final_combined_samples['ALT_SEQ_SCORE'] - final_combined_samples['SEQ_SCORE']\n",
        "roc_auc_score(final_combined_samples['INFO'], final_combined_samples['PLL_SCORE'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RRqO2z6xHFrt"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
